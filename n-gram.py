
""" 

https://huggingface.co/spaces/liujch1998/infini-gram



n-gram 是自然语言处理（NLP）和文本分析中的一个基本概念，用于表示文本中连续的n个单词（或字符）的序列。n-gram 模型在语言建模、文本生成、文本分类等任务中被广泛应用。以下是关于 n-gram 的详细介绍：

什么是 n-gram

	•	定义: n-gram 是文本中连续出现的 n 个词（或字符）的组合。根据 n 的值，可以分为不同的类型：
	•	Unigram (1-gram): 每个单词单独作为一个 gram。例如，文本 “I love NLP” 的 unigrams 是 [“I”, “love”, “NLP”]。
	•	Bigram (2-gram): 每两个连续的单词组成一个 gram。例如，“I love NLP” 的 bigrams 是 [“I love”, “love NLP”]。
	•	Trigram (3-gram): 每三个连续的单词组成一个 gram。例如，“I love NLP” 的 trigrams 是 [“I love NLP”]。

应用场景

	1.	语言建模: n-gram 模型用于预测文本序列中下一个单词的概率。大多数情况下，使用 bigram 或 trigram 模型。例如，在句子 “I love” 之后，“NLP” 出现的概率可以通过 bigram 模型进行预测。
	2.	文本生成: 使用 n-gram 模型生成新的文本，常见于自动摘要和对话生成系统。
	3.	文本分类: 将文本表示为 n-gram 特征向量，然后用于分类任务，例如情感分析、垃圾邮件检测等。
	4.	信息检索: 使用 n-gram 索引来提高搜索引擎的准确性和效率。
	5.	拼写检查: 通过 n-gram 统计检测和纠正拼写错误。

如何让小型语言模型高效工作。Yejin Choi在2024年数据与AI峰会上发表演讲（双语字幕）

演讲者：Yejin Choi，华盛顿大学教授、麦克阿瑟奖学金获得者，同时也是AI2常识AI的高级研究主任

这个演讲是将如何优化小模型的，他们训练了一个 0.5B 参数的小模型做文档摘要任务，超过了GPT-3.5。

按照演讲者的结论：AI 的性能，至少在目前的形态下，取决于它的训练数据。因此，过去和现在的 AI 主要依赖于人类生成的数据，但未来可能会依赖于 AI 生成的数据。很多可能担心合成数据质量不高，可能存在偏见，但是，越来越多的证据表明，这种方法是有效的。

例如，使用 Meta 的 SAM（Segment Anything）进行图像分割就是 AI 合成图像分割注释的一个例子。虽然有人类的验证帮助，但是单靠人类无法对如此多的图像样本进行注释。这是另一个例子。Microsoft 的论文"Textbooks are all you need"是另一个例证。当你有真正高质量的数据，例如教科书级别的数据，经过合成，你实际上可以在许多、许多不同的任务中与规模更大的对手竞争。可能在某些方面，它并不像大型模型那样具有广泛的适用性，但这对于满足许多商业需求来说是非常出色的。你可能不需要通才，你可能需要专家。

此外，"Textbooks are all you need"论文的观点也证明了，数据质量是最重要的。这并不仅仅关乎数量，更在于质量。DALL-3 就是一个很好的例子。为什么它会突然之间超越了 DALL-E 2 呢？很大程度上是因为它有更好的图像标注。但是，究竟是哪些更好的图像标注呢？在此之前，我们使用了所有好的图像标注。他们将这些图像标注进行了合成。这就是我们获得高质量数据的方式。

所以，AI 的质量更关乎数据的质量、新颖性和多样性，而不仅仅是数量。

* 引言

好的。我来这里要跟你们分享一些看似不可能实现的可能性。

* 介绍

去年，当有人问 Sam Altman 如何让印度的创业公司为印度创建基础模型时，他的回答是，不用费那个劲了。这根本没希望。

哇。首先，我希望印度的创业公司不会轻易放弃。其次，这种对话可能会在任何地方发生。无论是在美国的任何一所大学，或是创业公司，或是研究机构，他们都可能面临没有足够计算能力的问题。

所以，这就引出了我们要讨论的问题：不可能的提炼。如何以环保的方式训练出你的小型语言模型，使其效果堪比真实模型。

* 当前的方法与挑战

目前我们听到的最佳做法是大规模的预训练，紧接着进行大规模的后训练，例如 RLHF。如果我告诉你我打算从 GPT-2 开始，那个不被大众关注的小型、低质量模型，我也不知道为什么，但以某种方式，我们将创造或提炼出一个高质量的小型模型，然后与可能大两个数量级的更强大的模型竞争。这听起来应该很不可能，尤其是当你可能听说过一篇论文，标题是《模仿大型语言模型的虚假希望》。

虽然他们所报告的对于他们进行的特定评估实验来说是真实的，但请不要过分泛化，认为所有的小型语言模型都无法媲美大型模型。因为还有许多其他反例证明特定于任务的符号知识精馏可以奏效在许多不同的任务和领域中其中一些来自我的实验室。

然而今天，我只想关注一个任务即如何学习语言中的抽象。为了简化这个任务，我们从句子摘要开始这是我们的第一个不可能的任务。

* 任务一：句子摘要

目标是在没有极端规模的预训练、没有大规模的 RLHF、以及没有大规模的有监督数据集的情况下实现这一目标。这些东西并不总是必要的。但等一下，我们必须使用通常是所有三个，至少是其中一部分。

但如果没有这些，我们如何能和更大的模型一较高下呢？关键的直觉是当前的 AI 能做得多好取决于它所接受的训练数据。我们必须有某种优势。我们不能没有任何优势，所以那个优势将来自数据。顺便说一下，我们必须合成数据，因为如果数据已经存在于互联网上某处 OpenAI 已经对其进行了爬取，那就不是你的优势了，他们也有，所以你必须创造出一些真正新颖的东西，比现有的东西更好。

通常，精馏是从大型模型开始的，但我们将丢弃它，以向你展示我们可能对隐藏的可能性视而不见。所以我现在就开始示范。从 GPT-2 开始，那个质量很差的模型。然后我将进行一些创新，我马上就会概述，制作出高质量的数据集，然后可以用来训练小模型，这个模型将成为执行特定任务的强大模型。

但问题是，GPT-2 甚至无法理解你的提示词。你无法利用 GPT-2 进行提示词工程。你让它总结你的句子，它生成的一些输出，完全没有任何意义。所以你再试一次，因为它的输出通常有随机性。你可以生成很多不同的例子，比如几百个例子，我们发现它的表现几乎总是不好，像好的不到 0.1%。

* 解决方法与进展

但是有志者事竟成。所以我们想出了许多不同的办法。这其中包括我们的神经解码。这是一种即时推理算法，可以将任何逻辑约束加入到你的语言模型输出中。对于任何现成的模型，我们都可以使用这个来引导输出的语义空间。

但是因为 GPT-2 太糟糕了，即使使用了这个，成功率也只有 1%。但这比零要好。现在我们已经有了进展。因为如果你生成大量的样本，然后进行筛选，你实际上可以这样得到一些好的例子。然后，聪明的学生们提出了许多不同的想法。

我就不详述技术细节了，但我们找到了一些方法。为了能更容易找到好的例子，我们需要将成功的概率提高到 10% 以上。总体的流程是这样的：首先，从一个质量较差的教师模型开始，生成大量的数据点。然后，由于数据中存在大量的噪声，需要进行严格的过滤。

我们使用了一个三层过滤系统。虽然细节并不重要，但我要强调其中的第一个，即 Intel Monte 过滤器，它基于现成的 Intel Monte 分类器，能判断一个摘要是否能从原文中逻辑推断出来。这个模型并不完美，可能只有 70% 到 80% 的准确度。但是，当你大力使用它来过滤数据时，它的表现已经足够好了。

然后，我们使用这些数据来训练一个更小的模型，这个模型可以成为下一代学生的教师模型。我们重复了这个过程几次，最终得到了高质量的 DIMM sum 数据和高质量的模型。

在与那时最好的模型 GPT-3 进行对比时，那时，GPT-3 是最好的摘要模型。但当 ChatGPT 问世后，我们成功地超过了 GPT-3，人们似乎不再关心其他的，因为 ChatGPT 能做任何事情，包括摘要，所以我们为什么还要费心呢？

* 任务二：文档摘要

接下来是我们的“不可能完成的任务 2”。我们现在将与 ChatGPT 3.5 展开竞争。并且，要让我们的挑战更具难度——我们现在要总结的是整个文档，而不仅仅是句子。然后，我们还要做到以上所有这些而不依赖于那个现成的蕴含分类器。

我的意思是，实际上你可以这么做，就像从学术角度来看，我们想看看我们能在多大程度上打破关于规模的普遍假设。因此，我们在 InfoSumm 的新工作是一种基于信息理论的蒸馏方法其中关键的想法是我们将不再使用那个现成的 Imitating Humans LLM。我们将使用一些公式，这个公式其实只有三行，包括一些你可以用现成的语言模型来计算的条件概率得分。

* 实验与成果

现在时间还早，所以我们不深入讨论这些公式的细节。但我可以大体上告诉你，如果你把这些公式重新排列一下，你可以将此理解为点对点互信息的特例。你可以用它来过滤你的数据。因此，我们使用的是和之前相同的整体框架。我们现在使用 PTHEA 2.8 亿参数模型，因为我们觉得它比 GPT-2 稍好一些。

至于过滤，我们现在使用的是我之前向你们展示过的那三个简短的公式。然后我们就做同样的事情。这一次，我们让模型变得更小，只有 5 亿参数模型。这带来了高质量的文档摘要数据集，以及模型。

那么我们的表现如何呢？正如我们所承诺的，至少在这个任务上，我们的表现能与 ChatGPT-3.5 媲美，或者，根据评测的设定和标准，我们的表现甚至有所超越。你可以在我们的论文中找到更多的细节。

* 任务三：学习抽象思考

总的来说，我展示了我们如何学习文档摘要，即使不依赖于大规模预训练模型和其他大规模资源。然而，这两篇论文背后的真正研究问题是，我们如何学习进行抽象思考。

因为现在的做法就是让模型尽可能地大。越大越好。但是我们人类，无法像模型那样记住所有的上下文，比如一百万个 Token。没有人能记住上下文中的一百万个 Token。你会立刻抽象出我刚才告诉你的所有事情。但是你仍然记得我到目前为止说的所有话。这就是人类的惊人智能，我们还不知道如何通过 AI 模型有效地实现这一点。我相信这是可能的。我们只是还没有尽力去探索，因为我们被大规模的迷惑了。

* 任务四：Infini-gram

好的。那么，Infini-gram 就是我们面临的第三个挑战。稍微转换一下话题，现在的任务是让经典的统计 N-gram 语言模型在神经语言模型中发挥一定的作用。你们中有多少人还在讨论 n-gram 模型呢？我也不清楚。你们现在还在学习这个吗？

这里我们设定 n 等于无穷大。我们将在数万亿的 token 上完成这个计算，反应时间必须非常快，而且我们甚至不需要使用一颗 GPU。哇！我来告诉你们这有多么困难。假设，如果你要在一个经典的 n-gram 语言模型中索引 5 万亿个 token，且 n 无限大，那么你大概需要处理 2 千万亿个唯一的 n-gram 序列。你需要枚举，排序，计数，存储一些错误，这可能需要占用大约 32 太字节的硬盘空间，甚至更多。我们又怎么知道

呢？但这个数据量实在太大了。我们无法处理。

如果你看看其他人建立的大规模经典 N-gram 模型，那就是 Google。在 2007 年，由 Jeff Dean 和其他人带领的团队，他们只处理了 2 万亿个 token——我的意思是，对于那个时代来说，这已经是很大的数量了。他们使用的是五元 n-gram，这就产生了大约 3000 亿个不同的 n-gram 序列，这些序列他们都需要进行枚举、排序、计数等操作。这个数量实在是太庞大了。大家其实并没有进一步增加这个数量。那么，我们到底是如何做到将这个数量无限扩大的呢？

在我解释我们如何做到这一点之前，如果你感兴趣的话，我邀请你去查看这个在线演示。Infini-gram.io/demo。在这里，你可以搜索你想要的任何 token。这里有一个例子，它是一个 48 个字符的词。我不明白为什么这个词会存在。但是如果你去搜索它，你会发现它不仅存在，而且还有超过 3000 个实例。这个搜索过程耗时 5.5 毫秒。此外，它还会向你展示如何对这个长词进行分词。你也可以试试搜索多个词，看看下一个可能出现的词是什么。比如，"行动胜过语言"，那么接下来可能是什么词呢？该网站会向你展示可能出现的下一个词。而且，这个过程非常快速。

* 解决方法与进展

那么，我们是如何做到这一切的呢？你可能会惊讶地发现，我们的方法其实非常简单。有一种叫做后缀数组的数据结构，可能并不是所有的算法课程都会教授，但是有一些课程会教授。这是一种我们非常小心地实施的数据结构。所以我们用后缀数组索引整个网络语料库。事实上，我们并没有预先计算这些 n-gram 的统计数据。我们只是预先准备好这个数据结构。当你进行特定的查询时，我们会实时计算。多亏了这个数据结构——我们可以做得非常快，尤其是在使用 C++ 实现的情况下。我知道现在 AI 研究中，C++ 可能不是大家首选的语言，但实际上，使用 C++ 会让程序运行得更快。

这样做的成本有多低呢？其实我们只花了几百美元就索引了全部内容，而且，为 API 服务的成本也相当低。即便没有 GPU，它的速度也非常快。不同类型的 API 调用的延迟只有几十毫秒。你可以利用这个做很多事情。我现在可以分享的一点是，你可以用我们的 Infinigram 插值你的神经语言模型，降低困惑度，这是常用于评估语言模型质量的指标。我认为这只是我们能做的事情的冰山一角。实际上，我还在研究一些我希望能分享，但现在还不能告诉你们的东西。

不过我们已经开始提供这些 API 端点。从几周前开始计数，到现在我们已经提供了 6000 万次 API 调用，这还不包括我们自己的使用。我非常想知道人们是如何使用我们的 InfiniGram 的。

 


"""



from collections import defaultdict
import nltk
from nltk.util import ngrams

# 示例文档
documents = {
    1: "I love natural language processing",
    2: "Natural language processing is fascinating",
    3: "I enjoy learning about NLP and watching West World HBO show",
    4 : "Where is west world? Hello, the west world!",
}

# 创建倒排索引
index = defaultdict(list)

# 生成 n-gram 索引
n = 2  # 选择 bigram
for doc_id, text in documents.items():
    tokens = nltk.word_tokenize(text.lower())
    n_grams = ngrams(tokens, n)
    for gram in n_grams:
        index[gram].append(doc_id)

print("n-gram 索引:")
for key in index:
    print(key, index[key])


# 查询示例
query = "west world"
tokens = nltk.word_tokenize(query.lower())
query_grams = list(ngrams(tokens, n))

# 查询索引
results = defaultdict(int)
for gram in query_grams:
    if gram in index:
        for doc_id in index[gram]:
            results[doc_id] += 1

# 按匹配数量排序
sorted_results = sorted(results.items(), key=lambda item: item[1], reverse=True)

print("查询结果:")
for doc_id, count in sorted_results:
    print(f"Document ID: {doc_id}, Match Count: {count}")